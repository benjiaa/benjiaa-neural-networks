{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Azalea Yunus x Benji Andrews**\n",
    "\n",
    "Fall 2020\n",
    "\n",
    "CS 343: Neural Networks\n",
    "\n",
    "Project 2: Multi-layer Perceptrons\n",
    "\n",
    "**Submission reminders:**\n",
    "\n",
    "- Submit rubric on Google Classroom (one per team)\n",
    "- Submit one .zip file per team on Google Classroom. Includes:\n",
    "    - All .ipynb notebook files\n",
    "    - All .py code files\n",
    "    - Data files under 10 MB\n",
    "- Did you answer all 9 questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for obtaining the STL-dataset\n",
    "import load_stl10_dataset\n",
    "\n",
    "# for preprocessing dataset\n",
    "import preprocess_data\n",
    "\n",
    "# Set the color style so that Professor Layton can see your plots\n",
    "plt.style.use(['seaborn-colorblind', 'seaborn-darkgrid'])\n",
    "# Make the font size larger\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "# Turn off scientific notation when printing\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "\n",
    "# Automatically reload external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def plot_cross_entropy_loss(loss_history):\n",
    "    plt.plot(loss_history)\n",
    "    plt.xlabel('Training iteration')\n",
    "    plt.ylabel('loss (cross-entropy)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data\n",
    "\n",
    "### a. STL-10\n",
    "\n",
    "Run your function to load in the preprocessed STL-10 data in the following split:\n",
    "\n",
    "- 3500 training samples\n",
    "- 500 test samples\n",
    "- 500 validation samples\n",
    "- 500 samples for development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading stl10_binary.tar.gz 100.00%Downloaded stl10_binary.tar.gz\n",
      "Images are: (5000, 96, 96, 3)\n",
      "Labels are: (5000,)\n",
      "Resizing 5000 images to 32x32...Done!\n",
      "Saving Numpy arrays the images and labels to ./numpy...Done!\n",
      "shape test imgs (15, 3072)\n",
      "shape test labels (15,)\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "x_train, y_train, x_test, y_test, x_val, y_val, x_dev, y_dev = preprocess_data.load_stl10()\n",
    "test_imgs = x_dev[-15:,:]\n",
    "test_labels = y_dev[-15:]\n",
    "print(\"shape test imgs\", test_imgs.shape)\n",
    "print(\"shape test labels\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Circle in a square\n",
    "\n",
    "The circle in a square (CIS) dataset is a simple binary classification dataset that is useful for debugging and visualizing what your MLP is learning. Points with (x, y) coordinates inside a circle have class value of 1, points with coordinates outside the circle have class value of 0. Training on the CIS dataset allows us to answer the question: can the MLP discriminate whether a test point falls inside or outside the circle?\n",
    "\n",
    "#### Todo\n",
    "\n",
    "- Download the CIS dataset then run the cell below to load in the CIS train (`cis_train.dat`) and test (`cis_test.dat`) sets as numpy arrays.\n",
    "- Below, make a scatterplot showing the test set data. Color-code samples based on their class. If everything goes well, you should see a...solid, filled in circle inside unit square :)\n",
    "    - Make the aspect ratio of your x, y plotting axes equal, otherwiwse you might see an ellipse!\n",
    "\n",
    "In case you're curious about the data format:\n",
    "- Like usual, each row is a different sample.\n",
    "- The x-coordinate feature is the 1st column\n",
    "- The y-coordinate feature is the 2nd column\n",
    "- The class label (0 or 1) is in the third column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "data/cis/cis_train.dat not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-dd714a6c60dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcis_test_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cis'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cis_test.dat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcis_train_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcis_train_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# shuffle the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows)\u001b[0m\n\u001b[1;32m    959\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    533\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    534\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: data/cis/cis_train.dat not found."
     ]
    }
   ],
   "source": [
    "val_size = 20\n",
    "\n",
    "cis_train_path = os.path.join('data', 'cis', 'cis_train.dat')\n",
    "cis_test_path = os.path.join('data', 'cis', 'cis_test.dat')\n",
    "\n",
    "cis_train_all = np.loadtxt(cis_train_path, delimiter='\\t')\n",
    "\n",
    "# shuffle the data\n",
    "s_inds = np.arange(len(cis_train_all))\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(s_inds)\n",
    "\n",
    "cis_train_all = cis_train_all[s_inds]\n",
    "\n",
    "cis_train_x = cis_train_all[:, :2]\n",
    "cis_train_y = cis_train_all[:, 2].astype(int)\n",
    "\n",
    "cis_val_x = cis_train_x[:val_size]\n",
    "cis_train_x = cis_train_x[val_size:]\n",
    "cis_val_y = cis_train_y[:val_size]\n",
    "cis_train_y = cis_train_y[val_size:]\n",
    "\n",
    "cis_test_all = np.loadtxt(cis_test_path, delimiter='\\t')\n",
    "cis_test_x = cis_test_all[:, :2]\n",
    "cis_test_y = cis_test_all[:, 2].astype(int)\n",
    "\n",
    "print ('CIS Train data shape: ', cis_train_x.shape)\n",
    "print ('CIS Train labels shape: ', cis_train_y.shape)\n",
    "print ('CIS Validation data shape: ', cis_val_x.shape)\n",
    "print ('CIS Validation labels shape: ', cis_val_y.shape)\n",
    "print ('CIS Test data shape: ', cis_test_x.shape)\n",
    "print ('CIS Test labels shape: ', cis_test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot here\n",
    "plt.scatter(cis_test_all[:,0], cis_test_all[:,1], c=cis_test_all[:,2])\n",
    "plt.axis('equal')\n",
    "plt.title(\"CIS Test Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Implement Multilayer Perceptron (MLP) with softmax activation and cross-entropy loss\n",
    "\n",
    "Now that we've tested the softmax activation function and cross-entropy loss functions in a single-layer net, let's implement the MLP version.\n",
    "\n",
    "Much of your work on the single layer net will carry over, so go ahead and copy-paste and modify as needed!\n",
    "\n",
    "The structure of our MLP will be:\n",
    "\n",
    "```\n",
    "Input layer (X units) ->\n",
    "Hidden layer (Y units) with Rectified Linear activation (ReLu) ->\n",
    "Output layer (Z units) with softmax activation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Implement the following functions in `mlp.py`\n",
    "\n",
    "- `initialize_wts`\n",
    "- `accuracy`\n",
    "- `one_hot`\n",
    "- `predict`\n",
    "- `forward`\n",
    "- `backward`\n",
    "- `fit`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Test key functions with randomly generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy net for debugging\n",
    "num_inputs = 3\n",
    "num_features = 6\n",
    "num_hidden_units = 7\n",
    "num_classes = 5\n",
    "\n",
    "net = MLP(num_features, num_hidden_units, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random data and classes\n",
    "np.random.seed(0)\n",
    "test_x = np.random.normal(loc=0, scale=100, size=(num_inputs, num_features))\n",
    "test_y = np.random.uniform(low=0, high=num_classes-1, size=(num_inputs,))\n",
    "test_y = test_y.astype(int)\n",
    "print(f'Test input shape: {test_x.shape}')\n",
    "print(f'Test class vector shape: {test_y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `initialize_wts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.initialize_wts(M=num_features, H=num_hidden_units, C=num_classes, std=0.01)\n",
    "print(f'y wt shape is {net.y_wts.shape} and should be (6, 7)')\n",
    "print(f'y bias shape is {net.y_b.shape} and should be (7,)')\n",
    "print(f'z wt shape is {net.z_wts.shape} and should be (7, 5)')\n",
    "print(f'z bias shape is {net.z_b.shape} and should be (5,)')\n",
    "\n",
    "print(f'1st few y wts are\\n{net.y_wts[:,0]}\\nand should be\\n[ 0.018 -0.002  0.004  0.007  0.015  0.002]')\n",
    "print(f'y bias is\\n{net.y_b}\\nand should be\\n[-0.017  0.02  -0.005 -0.004 -0.013  0.008 -0.016]')\n",
    "print(f'1st few z wts are\\n{net.z_wts[:,0]}\\nand should be\\n[-0.002 -0.    -0.004  0.002  0.001  0.004  0.001]')\n",
    "print(f'z bias is\\n{net.z_b}\\nand should be\\n[ 0.015  0.019  0.012 -0.002 -0.011]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the `predict` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_pred = net.predict(test_x)\n",
    "print(f'Predicted classes are {test_y_pred} and should be [3 0 0]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the `forward` method focusing on`ReLU`(net act of hidden layer `y`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,y_net_act_test,_,_,_ = net.forward(test_x, test_y)\n",
    "\n",
    "correct_y_act = np.array([[7.643, 4.49 , 0.799, 9.977, 0.   , 0.   , 0.   ],\n",
    "       [2.353, 2.737, 2.175, 2.547, 0.345, 0.   , 0.   ],\n",
    "       [3.98 , 2.691, 1.19 , 3.029, 0.   , 0.   , 0.   ]])\n",
    "\n",
    "print(f'Your y activation is\\n{y_net_act_test}')\n",
    "print(f'The correct y activation (ReLU) is\\n{correct_y_act}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the `forward` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,_,probs,_ = net.forward(test_x, test_y)\n",
    "\n",
    "correct_probs = np.array([[0.219, 0.2  , 0.191, 0.219, 0.171],\n",
    "       [0.208, 0.204, 0.201, 0.205, 0.183],\n",
    "       [0.208, 0.202, 0.202, 0.205, 0.183]])\n",
    "\n",
    "print(f'Your z activation (class probabilities) is\\n{probs}')\n",
    "print(f'The correct z activation (class probabilities) is\\n{correct_probs}')\n",
    "print(f'The sums across rows (for each data sample) are {np.sum(probs, axis=1)}.')\n",
    "print(f'  You should know what should be :)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the `forward` method, focusing on loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_in, y_act ,z_in, z_act, loss = net.forward(test_x, test_y)\n",
    "correct_loss = 1.564402690536365\n",
    "\n",
    "print(f'Your average loss is\\n{loss}')\n",
    "print(f'The correct average loss is approx\\n{correct_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the `forward` method, focusing on regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_in, y_act ,z_in, z_act, loss = net.forward(test_x, test_y, reg=1000)\n",
    "correct_loss = 5.257207314928798\n",
    "\n",
    "print(f'Your regularized average loss is\\n{loss}')\n",
    "print(f'The correct regularized average loss is approx\\n{correct_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the `backward` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_in, y_act ,z_in, z_act, loss = net.forward(test_x, test_y, reg=0.5)\n",
    "grads = net.backward(test_x, test_y, y_in, y_act ,z_in, z_act, reg=0.5)\n",
    "\n",
    "print('Your gradient for y_wts is\\n', grads[0])\n",
    "print('Your gradient for y_b is\\n', grads[1])\n",
    "print('Your gradient for z_wts is\\n', grads[2])\n",
    "print('Your gradient for z_b is\\n', grads[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct gradients are:\n",
    "\n",
    "`\n",
    "Your gradient for y_wts is\n",
    " [[-0.476  0.057 -0.458 -0.115  0.03  -0.005  0.005]\n",
    " [-0.002  0.014 -0.046 -0.162  0.004  0.004  0.001]\n",
    " [-0.088  0.038 -0.166 -0.325 -0.001 -0.004 -0.013]\n",
    " [-0.331  0.067 -0.398 -0.332  0.001  0.    -0.001]\n",
    " [-0.318  0.089 -0.465 -0.615 -0.001 -0.01  -0.002]\n",
    " [-0.315 -0.036 -0.036  0.806  0.029 -0.005 -0.007]]\n",
    "Your gradient for y_b is\n",
    " [-0.005  0.    -0.004 -0.     0.     0.     0.   ]\n",
    "Your gradient for z_wts is\n",
    " [[-2.879  0.933  0.131  0.987  0.816]\n",
    " [-1.69   0.669 -0.261  0.699  0.584]\n",
    " [-0.374  0.278 -0.45   0.284  0.242]\n",
    " [-3.221  1.041  0.154  1.111  0.904]\n",
    " [ 0.024  0.027 -0.091  0.029  0.015]\n",
    " [ 0.002 -0.003 -0.004 -0.003 -0.002]\n",
    " [ 0.    -0.006  0.005  0.002 -0.008]]\n",
    "Your gradient for z_b is\n",
    " [-0.455  0.202 -0.135  0.209  0.179]\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test loss over epoch (1 of 2). \n",
    "\n",
    "The below code should generate a curve that rapidly drops to 0 (there might be fluctuations and it might not be monotonic and that's ok)\n",
    "\n",
    "Your `fit` function should show you print-outs showing:\n",
    "- Loss and validation accuracy 4 times throughout training.\n",
    "- 100% accuracy on validation set after around 5 epochs of training.\n",
    "- You are training on 20 epochs.\n",
    "- There are 20 iterations.\n",
    "- There is 1 iteration per epoch.\n",
    "\n",
    "Here is an example print-out from `fit`:\n",
    "\n",
    "    Starting to train network...There will be 20 epochs and 20 iterations total, 1 iter/epoch.\n",
    "     Completed Epoch 0/19. Training loss: 3.78. Validation accuracy: 33.33%.\n",
    "     Completed Epoch 5/19. Training loss: 0.13. Validation accuracy: 100.00%.\n",
    "     Completed Epoch 10/19. Training loss: 0.22. Validation accuracy: 100.00%.\n",
    "     Completed Epoch 15/19. Training loss: 0.15. Validation accuracy: 100.00%.\n",
    "    Finished training!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLP(num_features, num_hidden_units, num_classes)\n",
    "loss_hist, acc_train, acc_valid = net.fit(test_x, test_y, test_x, test_y, reg=0, print_every=5, lr=0.001, mini_batch_sz=3, n_epochs=20)\n",
    "\n",
    "print('\\nLengths of each output list:')\n",
    "print(f'{len(loss_hist)=}, {len(acc_train)=}, {len(acc_valid)=}')\n",
    "print('Each should be 20.')\n",
    "\n",
    "plot_cross_entropy_loss(loss_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test loss over epoch (2 of 2). \n",
    "\n",
    "The below curve should be more jagged that above, but still converge to 0 loss.\n",
    "\n",
    "Your `fit` function should print out:\n",
    "- Loss and validation accuracy 5 times throughout training.\n",
    "- 100% accuracy on validation set after around 4 epochs of training.\n",
    "- You are training on 10 epochs.\n",
    "- There are 30 iterations.\n",
    "- There are 3 iterations per epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLP(num_features, num_hidden_units, num_classes)\n",
    "loss_hist, acc_train, acc_valid = net.fit(test_x, test_y, test_x, test_y, reg=0, print_every=2, lr=0.001, mini_batch_sz=1, n_epochs=10)\n",
    "\n",
    "print('\\nLengths of each output list:')\n",
    "print(f'{len(loss_hist)=}, {len(acc_train)=}, {len(acc_valid)=}')\n",
    "print('The lengths should be 30, 10, 10.')\n",
    "\n",
    "plot_cross_entropy_loss(loss_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Test MLP with Circle in Square dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you run your MLP on the STL-10 dataset, test it out on the simpler CIS dataset.\n",
    "\n",
    "In cells below:\n",
    "- Train an MLP using the CIS training and validation sets. Configure the MLP with the following non-default hyperparameters:\n",
    "    - 50 hidden units\n",
    "    - Learning rate of 0.5\n",
    "    - Mini-batch size of 80\n",
    "    - 1000 epochs\n",
    "- Plot the loss over training iterations. You should see:\n",
    "    - A nice drop and plateau in mini-batch training loss.\n",
    "    - Accuracy on the validation set reach ~90%.\n",
    "- Create a scatterplot of the MLP predictions on the CIS test set. Color-code each sample by its class. Make sure your axis aspect ratios are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cis_net = MLP(2, 50, 2)\n",
    "loss, val, train = cis_net.fit(cis_train_x, cis_train_y, cis_val_x, cis_val_y, n_epochs=1000, lr=0.5, mini_batch_sz=80, verbose=1)\n",
    "plot_cross_entropy_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_pred = cis_net.predict(cis_test_x)\n",
    "plt.scatter(cis_test_all[:,0], cis_test_all[:,1], c=test_pred)\n",
    "plt.axis('equal')\n",
    "plt.title(\"MLP Predicted CIS Test Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5**: How do you interpret the circle-in-square scatterplot? Is the MLP doing a good job? \n",
    "\n",
    "**Question 6**: Play with\n",
    "- number of hidden units\n",
    "- number of epochs\n",
    "- batch size\n",
    "\n",
    "How does each parameter affect the results?\n",
    "\n",
    "**Question 7**: Do you think the single-layer net (with softmax) can handle the CIS dataset? Why or why not? (You're invited to try it, maybe as an extension :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decrease number of hidden units from 50 to 10\n",
    "cis_h = MLP(2, 10, 2)\n",
    "loss_cis_h, train_acc_cis_h, val_acc_cis_h = cis_h.fit(cis_train_x, cis_train_y, cis_val_x, cis_val_y, lr=0.5, mini_batch_sz=80, n_epochs=1000)\n",
    "# increase number of epochs from 1000 to 2500\n",
    "cis_e = MLP(2, 50, 2)\n",
    "loss_cis_e, train_acc_cis_e, val_acc_cis_e = cis_e.fit(cis_train_x, cis_train_y, cis_val_x, cis_val_y, lr=0.5, mini_batch_sz=80, n_epochs=2500)\n",
    "# decrease batch size from 80 to 10\n",
    "cis_b = MLP(2, 50, 2)\n",
    "loss_cis_b, train_acc_cis_b, val_acc_cis_b = cis_b.fit(cis_train_x, cis_train_y, cis_val_x, cis_val_y, lr=0.5, mini_batch_sz=10, n_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_cis_h)\n",
    "plt.xlabel('Training iteration')\n",
    "plt.ylabel('loss (cross-entropy)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_cis_e)\n",
    "plt.xlabel('Training iteration')\n",
    "plt.ylabel('loss (cross-entropy)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_cis_b)\n",
    "plt.xlabel('Training iteration')\n",
    "plt.ylabel('loss (cross-entropy)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_h = cis_h.predict(cis_test_x)\n",
    "plt.scatter(cis_test_all[:,0], cis_test_all[:,1], c=test_pred_h)\n",
    "plt.axis('equal')\n",
    "plt.title(\"MLP Predicted CIS Test Data - Fewer Hidden Layers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_e = cis_e.predict(cis_test_x)\n",
    "plt.scatter(cis_test_all[:,0], cis_test_all[:,1], c=test_pred_e)\n",
    "plt.axis('equal')\n",
    "plt.title(\"MLP Predicted CIS Test Data - More Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_pred_b = cis_b.predict(cis_test_x)\n",
    "plt.scatter(cis_test_all[:,0], cis_test_all[:,1], c=test_pred_b)\n",
    "plt.axis('equal')\n",
    "plt.title(\"MLP Predicted CIS Test Data - Smaller Batch Size\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 5**: I would say that the MLP is doing a reasonably good job, since the predicted shape resembles a circle in a square like we saw earlier. However, the circle is not perfectly round and symmetrical which indicates that the MLP could be performing better.\n",
    "\n",
    "**Answer 6**: By decreasing the number of hidden units and keeping the number of epochs and batch size the same, the prediction got worse as the circle became more angular and less round. By increasing the number of epochs, the prediction got better as the circle became more round. By decreasing the batch size, the prediction also got better and the circle became rounder.\n",
    "\n",
    "**Answer 7**: The single-layer network probably could not handle the CIS dataset if it used a softmax function. The ReLU function that we implemented in the first net activation is a more appropriate choice than the softmax function for the CIS dataset because of how each data sample is assigned one of two values. It doesn't make much sense to solely implement the softmax function a single-layer network for this dataset because each data sample only has its designated class as its sole feature. Furthermore, a nonlinear decision boundary is a task better suited to a net using the ReLU function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d. Test on STL-10 dataset, plot performance\n",
    "\n",
    "Train an MLP on the STL-10 training set with the following non-default hyperparameters:\n",
    "- 50 hidden units\n",
    "- Learning rate of 0.1\n",
    "- Regularization strength of 0.001\n",
    "- Mini-batch size of 500\n",
    "- 100 epochs\n",
    "    \n",
    "Make two plots:\n",
    "- Plot the training loss (like usual).\n",
    "- Plot the training and validation set accuracies (2 curves in one plot â€” include a legend, title, axis labels, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stl_net = MLP(3072, 50, 10)\n",
    "loss, val, train = stl_net.fit(x_train, y_train, x_val, y_val, n_epochs=100, lr=0.1, mini_batch_sz=500, print_every=5, verbose=1, reg=0.001)\n",
    "plot_cross_entropy_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val)\n",
    "plt.plot(train)\n",
    "plt.legend([\"validation set accuracy\",\"training set accuracy\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7**: What do the above loss and training and validation accuracy curves suggest about the quality of the hyperparameters used during training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 7:** The above figures show that the net is underfit and could use more training. This is evidenced by the somewhat linear decline in loss over iterations, and the higher validation set accuracy than training set accuracy with significant gap between the two.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3e. Optimize on STL-10 dataset with random search\n",
    "\n",
    "To optimize your MLP hyperparameters on STL-10, try a **random search** rather than a grid search. This means that instead of defining preset *values* that each hyperparameter takes on, define *ranges* (min and max values).\n",
    "\n",
    "Run your search for some $T$ iterations. On each iteration, randomly assign values to each hyperparameter within their valid ranges.\n",
    "\n",
    "Just like grid search, print out the accuracy and parameter values every time a bout of training yields the best accuracy on the STL-10 validation set. That way, if you need to stop the search prematurely, you know the current best parameter combination.\n",
    "\n",
    "Consider the following hyperparameters:\n",
    "- learning rate\n",
    "- regularization strength\n",
    "- number of hidden units\n",
    "- mini-batch size\n",
    "\n",
    "**Important note:** Like usual, I am not grading based on your performance numbers. I want to see that you successfully implemented the random search to find progressively better hyperparameters on STL-10.\n",
    "\n",
    "**Tip:** Just like with grid search, if you find a cluster of parameters that seems promising, you can revise your search to hone in on that smaller range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import multiprocessing as mp\n",
    "random.seed(0)\n",
    "\n",
    "def grid_search(lr_range, batch_range, reg_range, hiddenlayer_range, num_threads=5):\n",
    "    '''\n",
    "    a function to perform a grid search, finding best hyperparameters to train the net with.\n",
    "    lr_range/batch_range/reg_range: ndarray. length of each determines dimension of grid search.\n",
    "        these are best as output of np.linspace or similar functions.\n",
    "    num_threads: int\n",
    "    '''\n",
    "        \n",
    "    tic = time.time()\n",
    "    #set ranges\n",
    "\n",
    "    #assign lists to each of the threads\n",
    "    hyper_list = []\n",
    "    for i in range(num_threads):\n",
    "        hyper_list.append([])\n",
    "\n",
    "    count=0\n",
    "    for lr in lr_range:\n",
    "        for batch in batch_range:\n",
    "            for reg in reg_range:\n",
    "                    for hiddenlayer in hiddenlayer_range:\n",
    "                        hyper_list[count%num_threads].append([lr,batch,reg, hiddenlayer])\n",
    "                        count+=1\n",
    "    threadlist = []\n",
    "    for i in range(num_threads):\n",
    "        newT = mp.Process(target = train_thread, args = (i,hyper_list[i].copy()))\n",
    "        newT.start()\n",
    "        threadlist.append(newT)\n",
    "\n",
    "\n",
    "    for thread in threadlist:\n",
    "        thread.join()\n",
    "    toc = time.time()\n",
    "\n",
    "    print(f\"time was {toc-tic}\")\n",
    "                     \n",
    "def train_thread(net_id, queue):\n",
    "    '''\n",
    "    helper method grid search fn.\n",
    "    net_id: int. an id for the net so you can tell the threads apart in printouts\n",
    "    queue: python list of lists. each entry is a job to train the net on, format: [lr, batch size, reg, hiddenlayers]\n",
    "    '''\n",
    "    best_acc = 0\n",
    "    count = 0\n",
    "    for hyper_combo in queue:\n",
    "        net = MLP(3072, hyper_combo[3], 10)\n",
    "        loss_history = net.fit(x_train, y_train, x_val, y_val, print_every=100, n_epochs=100, verbose=1, lr=hyper_combo[0], mini_batch_sz=hyper_combo[1], reg=hyper_combo[2])\n",
    "        val_acc = net.accuracy(y_val, net.predict(x_val))\n",
    "          \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            print(f\"net {net_id} found a new best accuracy on validation set.\\n validation set accuracy = {val_acc} \")\n",
    "            print(f\"hyperparameters: \\n LR = {hyper_combo[0]} \\n batch size = {hyper_combo[1]} \\n reg = {hyper_combo[2]} \\n num hidden = {hyper_combo[3]}\")\n",
    "        print(f\"net {net_id} finished run {count} \")\n",
    "        count+=1\n",
    "    print(\"-------------------------------------------------\")                        \n",
    "    print(f\"NET {net_id} BEST OVERALL ACC: {best_acc}\\n\")    \n",
    "    print(f\"hyperparameters: \\n LR = {hyper_combo[0]} \\n batch size = {hyper_combo[1]} \\n reg = {hyper_combo[2]}\")\n",
    "    print(\"-------------------------------------------------\")                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "#passing random values into grid search function\n",
    "\n",
    "lr = np.random.uniform(low=.05, high=.15, size=(3))\n",
    "batch = np.random.uniform(low=300, high=600, size=(3)).astype(\"int\")\n",
    "reg = np.random.uniform(low=0.0001, high=0.0001, size=(3))\n",
    "hidden = np.random.uniform(low=35, high=65, size=(3)).astype(\"int\")\n",
    "\n",
    "grid_search(lr, batch, reg, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net 2 found a new best accuracy on validation set.\n",
    "#  validation set accuracy = 0.438 \n",
    "# hyperparameters: \n",
    "#  LR = 0.12151893663724195 \n",
    "#  batch size = 463 \n",
    "#  reg = 0.0001 \n",
    "#  num hidden = 46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3f. Plot STL-10 results with best hyperparameters\n",
    "\n",
    "Train an MLP with the best hyperparameters that you found from your parameter search and create two plots:\n",
    "- Training STL-10 loss curve\n",
    "- Training and validation set STL-10 accuracy curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "bestNet = MLP(3072, 20, 10)\n",
    "loss_history, train_history, val_history = bestNet.fit(x_train, y_train, x_val, y_val, n_epochs= 200, verbose=1, print_every=100, lr=0.1215, mini_batch_sz=463, reg= .0001)\n",
    "\n",
    "test_acc = bestNet.accuracy(y_test, bestNet.predict(x_test))\n",
    "plot_cross_entropy_loss(loss_history)\n",
    "\n",
    "plt.plot(val_history)\n",
    "plt.plot(train_history)\n",
    "plt.legend([\"validation set accuracy\",\"training set accuracy\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "plt.show()\n",
    "print(f\"test acc is {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8**: Use your best trained network to compute the accuracy on the test set after 200 epochs. What accuracy do you get?\n",
    "\n",
    "**Question 9:** Why would you use random search over grid search when optimizing parameters on a dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 8**: The test set accuracy computed on the test set is ~38%.\n",
    "\n",
    "**Answer 9:** A random search is better when some hyperparameters are not as important as others. Selecting randomly gives high variance but can occasionally result in significantly better performances by chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3g. Visualize learned weights\n",
    "\n",
    "Run the `plot_weights` function to generate a grid visualization of them.\n",
    "\n",
    "You should see structure in the weights if your network is performing well. If you have a large number of hidden units, some may not be \"used\" so a subset of the weights may resemble \"noise\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_y_wts = bestNet.get_y_wts()\n",
    "best_y_wts = best_y_wts.reshape(32, 32, 3, -1).transpose(3, 0, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights(wts, maxRows=25, verbose=0):\n",
    "    # limit height of figure by number of neurons\n",
    "    grid_sz = int(maxRows)\n",
    "    grid_sz = np.minimum(grid_sz, int(np.sqrt(len(wts))))\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(f'Showing {grid_sz} rows')\n",
    "    \n",
    "    plt.figure(figsize=(20,20))\n",
    "    for x in range(grid_sz):\n",
    "        for y in range(grid_sz):\n",
    "            lin_ind = np.ravel_multi_index((x, y), dims=(grid_sz, grid_sz))\n",
    "            plt.subplot(grid_sz, grid_sz, lin_ind+1)\n",
    "            currImg = wts[lin_ind]\n",
    "            low, high = np.min(currImg), np.max(currImg)\n",
    "            currImg = 255*(currImg - low) / (high - low)\n",
    "            currImg = currImg.astype('uint8')\n",
    "            plt.imshow(currImg)\n",
    "            plt.gca().axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weights(best_y_wts, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "**Reminder**: Please do not integrate extensions into your base project so that it changes the expected behavior of core functions. It is better to duplicate the base project and add features from there.\n",
    "\n",
    "1) Analyze the differences between training when sampling with replacement (i.e. not every input sample is usually processed on an epoch) and sampling without replacement (e.g. time, accuracy, loss, etc).\n",
    "\n",
    "2) Investigate how the single layer softmax network does with the CIS dataset. Explain and provide plots showing your results.\n",
    "\n",
    "3) If you have time to spare (or want to throw more computing power at the STL-10 dataset), process through the SLP and MLP and tune hyperparameters with the dataset at its original resolution (96x96 images). Show images of your learned weights. Can you find a training sweet spot where the learned weight visualizations look particularly cool?\n",
    "\n",
    "4) Implement a multi-class sigmoid classifer. I suggest creating another subclass of `SoftmaxLayer` and/or `MLP`. Compare and contrast results achieved by the softmax network.\n",
    "\n",
    "5) Explore alternative MLP architectures and compare/constrast results and performance with the ones used in the base project. For example, replace hidden layer activation function with sigmoid, add one or more additional hidden layers, etc. \n",
    "\n",
    "6) Explore the effects of batch gradient descent, stochastic gradient descent, and mini-batch gradient descent. Make plots and interpret your results.\n",
    "\n",
    "7) Obtain, preprocess, train, and evaluate the performance of `SoftmaxLayer` and/or `MLP` on another dataset with comparable types of image features. MNIST is a good one.\n",
    "\n",
    "8) Make a fancy coarse-to-fine grid search that automatically \"zooms in\" on the best hyperparameter combination ranges several times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extension: Multithreaded Grid Search and Random Search**\n",
    "\n",
    "This extension is baked into the project notebooks, but not the base code. In order to do a more efficient grid/random search, I implemented a multiprocess solution with python's multiprocessing library. This library enables us to write functions that can be executed concurrently. Grid searching is especially well cut out for optimization via concurrency due to the results of each run of the net not directly depending on one another. We do of course have to eventually compare validation set accuracy across threads, but this can be done by the user by printing out the lifetime best accuracy of each thread as you go along. \n",
    "\n",
    "It took a few iterations to come up with this solution, but eventually, I created logic in a grid search function that takes in numpy linspaces with the desired number of parameters entered already. This way, the user can enter parameters in a familiar way. The grid search function creates a queue of jobs to be executed, (pretty much) evenly distributed over the appopriate number of threads. Then, the specified number of threads is initialized (I had luck with 5), and each thread works through its individual queue at its own pace, printing best accuracy values as it goes. The threads are joined at the end. The result is a grid search function that uses as significantly higher percentage of my CPU while running and works faster than my single threaded program by a factor of roughly 2 to 5 depending on the specific parameters. \n",
    "\n",
    "Adapting the grid search function for random searching was as simple as changing the expected linspaces to random uniform 1D arrays within given high and low ranges, as well as adding an additional parameter due to the number of hidden layers in an MLP.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for obtaining the STL-dataset\n",
    "import load_stl10_dataset\n",
    "\n",
    "# for preprocessing dataset\n",
    "import preprocess_data\n",
    "\n",
    "from softmax_layer import SoftmaxLayer\n",
    "\n",
    "# Set the color style so that Professor Layton can see your plots\n",
    "plt.style.use(['seaborn-colorblind', 'seaborn-darkgrid'])\n",
    "# Make the font size larger\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "# Turn off scientific notation when printing\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "\n",
    "# Automatically reload external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "def plot_cross_entropy_loss(loss_history):\n",
    "    plt.plot(loss_history)\n",
    "    plt.xlabel('Training iteration')\n",
    "    plt.ylabel('loss (cross-entropy)')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get CIS data from base project\n",
    "val_size = 20\n",
    "\n",
    "cis_train_path = os.path.join('data', 'cis', 'cis_train.dat')\n",
    "cis_test_path = os.path.join('data', 'cis', 'cis_test.dat')\n",
    "\n",
    "cis_train_all = np.loadtxt(cis_train_path, delimiter='\\t')\n",
    "\n",
    "s_inds = np.arange(len(cis_train_all))\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(s_inds)\n",
    "\n",
    "cis_train_all = cis_train_all[s_inds]\n",
    "\n",
    "cis_train_x = cis_train_all[:, :2]\n",
    "cis_train_y = cis_train_all[:, 2].astype(int)\n",
    "\n",
    "cis_val_x = cis_train_x[:val_size]\n",
    "cis_train_x = cis_train_x[val_size:]\n",
    "cis_val_y = cis_train_y[:val_size]\n",
    "cis_train_y = cis_train_y[val_size:]\n",
    "\n",
    "cis_test_all = np.loadtxt(cis_test_path, delimiter='\\t')\n",
    "cis_test_x = cis_test_all[:, :2]\n",
    "cis_test_y = cis_test_all[:, 2].astype(int)\n",
    "\n",
    "softmax_cis = SoftmaxLayer(2)\n",
    "loss_history = softmax_cis.fit(cis_train_x, cis_train_y, n_epochs=100, mini_batch_sz=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss over time\n",
    "plot_cross_entropy_loss(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions\n",
    "test_pred_sm = softmax_cis.predict(cis_test_x)\n",
    "plt.scatter(cis_test_all[:,0], cis_test_all[:,1], c=test_pred_sm)\n",
    "plt.axis('equal')\n",
    "plt.title(\"Softmax Predicted CIS Test Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this extension, we tested out the softmax single-layer network with the CIS dataset. The loss graph demonstrates that the loss does not decrease over time, and it fluctuates between increasing and decreasing. Additionally, we see that the softmax network does not predict any points as part of a circle, and so we were correct in our prediction that the softmax single-layer network would be an inapropriate choice for the CIS dataset. Since there is no hidden layer performing ReLU activation, the network simply sees the binary classification of the CIS data but does not understand how the individual points that are of the 'circle' class connect with one another to draw a circle in a square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp import MLP\n",
    "from sklearn.datasets import fetch_openml\n",
    "iris_full=fetch_openml(data_id=61)\n",
    "mnist_full = fetch_openml('mnist_784')\n",
    "\n",
    "#MNIST import\n",
    "mnist = mnist_full.data.astype(np.float64)\n",
    "mnist_classes = mnist_full.target.astype(np.int)\n",
    "mnist_dev = mnist[:1000]\n",
    "mnist_dev_classes = mnist_classes[:1000]\n",
    "x_train_mnist, y_train_mnist, x_test_mnist, y_test_mnist, x_val_mnist, y_val_mnist, x_dev_mnist, y_dev_mnist = preprocess_data.create_splits(mnist, mnist_classes, 40000, 20000, 5000, 5000)\n",
    "\n",
    "#standardizing\n",
    "mnist_dev_std = (mnist_dev- np.mean(mnist_dev))/np.std(mnist_dev)\n",
    "mnist_dev_std = (mnist_dev_std - np.mean(mnist_dev_std,axis=0))/np.std(mnist_dev_std,axis=0)\n",
    "\n",
    "softmax_mnist = SoftmaxLayer(10)\n",
    "#print(x_train_mnist.shape)\n",
    "#print(y_train_mnist.shape)\n",
    "#print(mnist_classes)\n",
    "loss_history_mnist = softmax_mnist.fit(x_train_mnist, y_train_mnist, n_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss over time\n",
    "plot_cross_entropy_loss(loss_history_mnist)\n",
    "print(\"First loss:\", loss_history_mnist[0])\n",
    "print(\"First loss:\", loss_history_mnist[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this extension, we tested out the softmax single-layer network with the MNIST dataset of digits The loss graph demonstrates that the loss did decrease over time from about 30 to 0.72. The decrease wasn't linear, which suggests that different hyperparameters might be used to acheive a less noisy loss graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
